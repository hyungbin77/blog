
# 9-1
## 9-1-1 주성분 분석 소개 1

- ### 차원 축소의 필요성 
	- 최근에 모아지고 분석의 대상이 되는 자료들은 대부분 고차원인 경우가 많음
	- 즉, 하나의 대상에 특성치가 많은 경우가 대부분임
	- 이렇게 모아진 자료들은 특성치들 간의 연관성이 강한 경우가 많으며 이를 파악하는 것은 자료 분석과 예측에서 많은 난관을 주거나 해석이나 이해를 너무 어렵게 할 수 있음
	- 이로부터 고차원 데이터를 중요한 몇 개의 축약변수를 찾아내는 것을 차원 축소 기법이라 함
	
- ### 선형 변환 및 변동 유지 기법으로 주성분 분석
	- 주성분 분석은 고차원 특성 변수들의 선형결합으로 중요한 특성을 가지는 축약변수를 만들어
	  내는 기법 또는 알고리즘 
	- 자료의 정보는 여러 측면에서 평가할 수 있으며, 이 중 변동을 주요하게 고려함
	- 그렇다면 새로운 축약변수는 변동을 줄어드는 것이 아니라 최대의 변동치를 가지는 것을 찾는
	  것이 바람직
	- 주성분 분석은 선형변환을 통해서 가장 큰 변동이 나타나는 축약변수를 찾는 방법이며 첫 번째
	 축약변수가 찾아지면 이와 선형독립인 축에서 가장 큰 변동이 나타나는 축약변수를 순차적으로 찾는 알고리즘 
	- 만약에 p개의 차원이 있는 특성변수들이 있다면, 정확히 그 개수만큼의 축약변수를 고려할 수 있으며, 이중 변동설명력이 높아서 전체 변동의 대부분이 설명되는 소수의 축약변수를 찾는 것을     목표로 함 



- ### 성질
	- 만약에 특성치들의 상관관계가 낮다면 주성분 분석은 크게 도움이 안될 수도 있음
	  상관관계가 높거나 모은 자료의 개수에 비해서 특성치의 차원이 매우 크다면 주성분 분석은 매우 유요한 방법론이 됨 


## 9-1-2 주성분 분석 소개 2

- ### 주성분의 의미 
	- 주성분들은 기본적으로 직교성질을 가지고 있음, 즉 현재 변수가 $x_{1} ,..., x_{p}$ 라고 할 때 주성분 변수   $pc_{1},...,pc_{m}$ 은 어떤 상에 대해서도 그 공분산이 0이 됨 
	- 이런 직교 성질은 주성분들이 가지는 장점이기도 하고 단점이기도 함, 즉 직교성질로부터 여러 가지 수학적 성질이 단순하고 설명가능성이 높다는 점은 큰 장점이 되나 실제 현실에서는 반드시 차원 축소로 얻어진 주성분이 직교일 필요는 없는 경우가 있음 

> [!quote]- 주성분 그림
![[스크린샷 2024-11-02 오후 8.35.54.png|500]]

- ### 주성분 분석 플랏 
	- 주성분은 보통 원래 변수의 차원보다 작다고 가정하며, 주성분을 통한 분석을 위해서는 기본적인 성질을 이해해야 함
	- 그림으로 표현할 수 있는 주성분은 보통 2개인 경우가 많음, 이로부터 보통 주성분 1과 2(가장 변동이 큰 성분과 다음으로 변동이 큰 성분)의 값을 그림으로 확인해 보는 경우가 많음
	- 보통 살펴보는 것은 주성분 1과 주성분 2의 변동의 정도 및 각각의 점들을 라벨링했을 때, 특별한 특성을 나타내는지 임(여기에서 둘 간의 선형관계가 관측되면 제대로 된 주성분들이 아님에 주의)
## 9-1-3 주성분 분석 수학

- ### 스펙트럴 분해
	- 정방행렬이 있어 대칭이고(열과 행을 바꾸어도 같은 값) 양정치 행렬이라면(0벡터가 아닌 어떤 벡터의 전치행렬과 해당 정방행렬을 곱하여 다시 해당 벡터를 곱할 때 양수가 나옴), 아래와 같은 행렬이 분해가 됨 
	- 고유벡터를 열로 가지는 정방행렬(교유벡터는 노음이 1이며 서로 직교함)과 대각행렬(대각원소는 고유값을 가지며 나머지 원소는 0)과 앞의 정방행렬의 전치행렬, 이렇게 세 개 행렬의 곱으로 분해가 됨 $X\Lambda X^{T}$
	- 이것은 다시 고유값과 고유벡터 자체의 외적을 곱하고 이를 합하는 것으로 표현이 가능함,
	  참고로 고유값은 다 양수임


- ### 해석
	- 가장 큰 고유값에 해당하는 고유벡터는 분산을 가장 크게 만드는 축 변환을 가능하게함
	- 참고로 원자료를 $N\times P$  행렬로 나타낼 때, 전치행렬과 원행렬의 곱을(평균이 0이라고 가정)
	  샘플 수(또는 샘플 수 - 1) 로 나누어 공분산 행렬을 만들어서 스펙트럴 분해를 하게 됨 
	- 이 경우 원자료의 특성값들에 고유벡터를 내적하면 새로운 축의 좌표값이 나오게됨
	- 새로운 좌표축들은 모두 다 독립이며 새로운 기저가 됨
	- 참고로 특성치의 차원이 p라면 고유벡터도 p개가 존재한다 

- ### 성질
	- 행렬식은 고유값의 곱이 된다. 이로부터 고유값이 0이 되는 상황(반양정치의 경우 가능)이  있을 수 있음
	- $$det(A) = \lambda_{1}\lambda_{2}...\lambda_{n} \;\text{ if }  \;\lambda_{n} = 0$$
	  $$det(A) = 0$$
	- 트레이스는 고유값의 합이 된다/ 즉 고유값의 합은 원자료 분산의 합과 같음
	- 일반적으로 주성분 분석에서는 몇 개의 주성. 이 분산합의 몇 %를 유지하는가를 주로 살펴보고 
	  그 개수를 정함

## 9-1-4 주성분 분석 방법론


- ### 주성분을 찾는 법 (1)
	- 공분산 행렬 내지 상관계수 행렬에 대한 스펙트럴 분해를 시행
	- 어느 것을 사용하는 가에 따라 결과가 다르게 나오며, 보통 심한 분산의 편차로부터 오는 편향을   제거하기 위해서 상관계수 행렬을 사용하는 것이 일반적임
	- 위에서 정한 행렬에 대한 스펙트럴 분해를 통해서  첫 번째 고유벡터(가장 큰 고유값과 대응)가 나오면 자료의 특성벡터와 고유벡터의 내적을 통해서 첫번째 주성분을 구함/ 이 주성분의 분산은 고유값이 됨
	- 다음 고유벡터와의 내적을 통해서 두 번째 주성분을 얻음, 이런 방식으로 여러 개의 주성분을 얻을 수 있음
- ### 주성분을 찾는 법(2)
	1. 평균 계산 
	2. 데이터 중심화(Centering) : 각 데이터에서 평균을 빼서 중심으로 데이터를 모음. 이는 센터링하지 않았을 때, 주성분 방향이 일치하지 않아 데이터의 분포가 이상해지기 때문
	3. 공분산(Covariance) 행렬 계산
	4. 고유값과 고유벡터 계산 : 공분산 행렬에 decomposition을 적용해 고유값(Eigen value)과 고유벡터(Eigenvvector)를 구함
	5. 주성분 선택 : 가장 큰 고유값을 갖는 고유벡터로 부터 원하는 수의 주성]]분을 선택                  [[PCA(주성분 분석)#^principal-component-selection|cf) 주성분 수 선택 방법]]


- ### 몇개의 주성분을 얻을지 선택하는 법 ^principal-component-selection
	- 일반적으로 상관정도가 강하다면 고유값을 큰 값에서 적은 값으로 누적해서 합을 구해가면 뒤쪽으로 가면 갈수록 합의 증가속도가 느려짐
	- 이로부터 증가 속도가 작아지는 지점(고유값 기준으로 감소 속도가 느려지는 지점)을 선택함
	- 보통 주성분을 얻어서 시각화를 할 때에는 주성분1과 주성분2를 살펴보는 경우가 많음

- ### 주성분의 정보에 대한 평가
	- 주성분의 개수가 q개일 때, 고유값 중 q개의 최대치 합과 전체 변수 분산합을 비교해서 분산 설명 비율을 계산함
	  $$\frac{\Sigma_{i = 1}^{q}\lambda_{i}}{\Sigma_{j = 1}^{p}\lambda_{j}}$$
	- p 개의 특성변수에서 p개의 주성분들이 나온다고 할 때, 주성분이 선형변환에서 얻어지는 특성으로 인하여 p보다 적은 주성분으로 원 특성변수 공간으로 변환이 가능함
	- 위 변환에서 원래 특성값과 차이를 보이게 되며, 이는 손실로 평가
	- 데이터 자체의 연관성이 높거나 잡음이 많다면 적은 수의 주성분 내지 변환으로 회귀분석, 분류에서 높은 성능을 얻을 수 있다는 것에 주의, 이를 주성분 회귀분석이라고도 함 
